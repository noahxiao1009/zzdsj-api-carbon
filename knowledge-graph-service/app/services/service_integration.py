"""
çŸ¥è¯†å›¾è°±æœåŠ¡çš„æœåŠ¡é—´é€šä¿¡é›†æˆ
åŸºäºåŸå§‹é¡¹ç›®çš„çŸ¥è¯†å›¾è°±å®ç°ï¼Œæä¾›å®ä½“å…³ç³»æŠ½å–ã€å›¾è°±æ„å»ºã€æ¨ç†æŸ¥è¯¢ç­‰åŠŸèƒ½
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional, Union, Tuple
from datetime import datetime, timedelta
import json
import sys
import os
import uuid
import re
from collections import defaultdict

# æ·»åŠ sharedæ¨¡å—åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), "../../../shared"))

from service_client import (
    ServiceClient, 
    AsyncServiceClient,
    CallMethod, 
    CallConfig, 
    RetryStrategy,
    ServiceCallError,
    call_service,
    publish_event
)

logger = logging.getLogger(__name__)


class GraphProcessor:
    """å›¾è°±å¤„ç†å™¨"""
    
    def __init__(self):
        self.entity_types = {
            "PERSON": "äººç‰©",
            "ORGANIZATION": "ç»„ç»‡",
            "LOCATION": "åœ°ç‚¹", 
            "EVENT": "äº‹ä»¶",
            "CONCEPT": "æ¦‚å¿µ",
            "TIME": "æ—¶é—´",
            "NUMBER": "æ•°å€¼"
        }
        
        self.relation_types = {
            "WORKS_FOR": "ä¾›èŒäº",
            "LOCATED_IN": "ä½äº",
            "PART_OF": "éš¶å±äº",
            "RELATED_TO": "ç›¸å…³",
            "CAUSED_BY": "ç”±äº",
            "LEADS_TO": "å¯¼è‡´"
        }
    
    def standardize_entity(self, entity: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–å®ä½“æ ¼å¼"""
        return {
            "id": entity.get("id", str(uuid.uuid4())),
            "text": entity.get("text", ""),
            "type": entity.get("type", "CONCEPT"),
            "confidence": entity.get("confidence", 0.8),
            "properties": entity.get("properties", {}),
            "aliases": entity.get("aliases", [])
        }
    
    def standardize_relation(self, relation: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–å…³ç³»æ ¼å¼"""
        return {
            "id": relation.get("id", str(uuid.uuid4())),
            "source": relation.get("source", ""),
            "target": relation.get("target", ""),
            "type": relation.get("type", "RELATED_TO"),
            "confidence": relation.get("confidence", 0.7),
            "properties": relation.get("properties", {})
        }


class KnowledgeGraphServiceIntegration:
    """çŸ¥è¯†å›¾è°±æœåŠ¡é›†æˆç±» - æ™ºèƒ½å®ä½“å…³ç³»æŠ½å–å’Œå›¾è°±æ„å»º"""
    
    def __init__(self):
        self.service_client = None
        self.async_client = None
        self.graph_processor = GraphProcessor()
        
        # ä¸åŒæ“ä½œçš„é…ç½®
        self.model_config = CallConfig(
            timeout=120,  # æ¨¡å‹è°ƒç”¨éœ€è¦æ›´é•¿æ—¶é—´
            retry_times=2,
            retry_strategy=RetryStrategy.EXPONENTIAL,
            circuit_breaker_enabled=True
        )
        
        self.knowledge_config = CallConfig(
            timeout=30,   # çŸ¥è¯†åº“æ“ä½œ
            retry_times=2,
            retry_strategy=RetryStrategy.LINEAR
        )
        
        self.database_config = CallConfig(
            timeout=20,   # å›¾æ•°æ®åº“æ“ä½œ
            retry_times=3,
            retry_strategy=RetryStrategy.LINEAR
        )
        
        self.auth_config = CallConfig(
            timeout=5,    # æƒé™æ£€æŸ¥è¦å¿«
            retry_times=2,
            retry_strategy=RetryStrategy.LINEAR
        )
        
        # æ”¯æŒçš„çŸ¥è¯†å›¾è°±åŠŸèƒ½
        self.graph_capabilities = {
            "entity_extraction": {
                "description": "å®ä½“æŠ½å–",
                "models": ["bert-ner", "spacy-ner", "custom-ner"]
            },
            "relation_extraction": {
                "description": "å…³ç³»æŠ½å–", 
                "models": ["bert-relation", "gpt-relation", "rule-based"]
            },
            "graph_reasoning": {
                "description": "å›¾è°±æ¨ç†",
                "algorithms": ["path-finding", "community-detection", "centrality"]
            },
            "knowledge_fusion": {
                "description": "çŸ¥è¯†èåˆ",
                "strategies": ["entity-linking", "conflict-resolution", "trust-propagation"]
            }
        }
        
        # å¤„ç†ç»Ÿè®¡
        self.processing_stats = {
            "total_documents": 0,
            "total_entities": 0,
            "total_relations": 0,
            "processing_time": 0.0
        }
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.service_client = ServiceClient()
        self.async_client = AsyncServiceClient()
        await self.service_client.__aenter__()
        await self.async_client.__aenter__()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.service_client:
            await self.service_client.__aexit__(exc_type, exc_val, exc_tb)
        if self.async_client:
            await self.async_client.__aexit__(exc_type, exc_val, exc_tb)
    
    # ==================== å®ä½“å’Œå…³ç³»æŠ½å– ====================
    
    async def extract_entities_from_text(
        self, 
        text: str, 
        user_id: str,
        model: str = "bert-ner",
        confidence_threshold: float = 0.7
    ) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“ï¼ˆè°ƒç”¨æ¨¡å‹æœåŠ¡ï¼‰"""
        try:
            # æƒé™æ£€æŸ¥
            permission_check = await self._check_graph_permission(user_id, "entity_extraction")
            if not permission_check.get("allowed"):
                return {
                    "success": False,
                    "error": "æƒé™ä¸è¶³",
                    "required_permission": "knowledge_graph:entity_extraction"
                }
            
            logger.info(f"å¼€å§‹ä»æ–‡æœ¬æå–å®ä½“ï¼Œç”¨æˆ·: {user_id}, æ¨¡å‹: {model}")
            
            # è°ƒç”¨æ¨¡å‹æœåŠ¡è¿›è¡Œå®ä½“æŠ½å–
            result = await self.service_client.call(
                service_name="model-service",
                method=CallMethod.POST,
                path="/api/v1/nlp/entity-extraction",
                config=self.model_config,
                json={
                    "text": text,
                    "model": model,
                    "user_id": user_id,
                    "language": "zh",
                    "confidence_threshold": confidence_threshold
                }
            )
            
            raw_entities = result.get("entities", [])
            
            # æ ‡å‡†åŒ–å®ä½“æ ¼å¼
            entities = []
            for entity in raw_entities:
                if entity.get("confidence", 0) >= confidence_threshold:
                    standardized = self.graph_processor.standardize_entity(entity)
                    entities.append(standardized)
            
            # æ›´æ–°ç»Ÿè®¡
            self.processing_stats["total_entities"] += len(entities)
            
            logger.info(f"æå–åˆ° {len(entities)} ä¸ªå®ä½“ (é˜ˆå€¼: {confidence_threshold})")
            
            return entities
            
        except ServiceCallError as e:
            logger.error(f"å®ä½“æå–å¤±è´¥: {e}")
            if e.status_code == 503:
                # æœåŠ¡ä¸å¯ç”¨ï¼Œä½¿ç”¨è§„åˆ™æå–
                return await self._fallback_entity_extraction(text)
            raise
        except Exception as e:
            logger.error(f"å®ä½“æå–å¼‚å¸¸: {e}")
            raise
    
    async def build_knowledge_graph_workflow(
        self, 
        document_id: str, 
        user_id: str,
        extraction_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """æ„å»ºçŸ¥è¯†å›¾è°±å®Œæ•´å·¥ä½œæµ"""
        try:
            start_time = datetime.now()
            logger.info(f"å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œæ–‡æ¡£: {document_id}, ç”¨æˆ·: {user_id}")
            
            # æƒé™æ£€æŸ¥
            permission_check = await self._check_graph_permission(user_id, "graph_building")
            if not permission_check.get("allowed"):
                return {
                    "success": False,
                    "error": "æƒé™ä¸è¶³",
                    "required_permission": "knowledge_graph:graph_building"
                }
            
            # 1. è·å–æ–‡æ¡£å†…å®¹
            document_result = await self._get_document_content(document_id, user_id)
            if not document_result.get("success"):
                return document_result
            
            document = document_result["data"]
            document_content = document.get("content", "")
            
            if not document_content.strip():
                return {
                    "success": False,
                    "error": "æ–‡æ¡£å†…å®¹ä¸ºç©º",
                    "document_id": document_id
                }
            
            # 2. å¹¶å‘æ‰§è¡Œå®ä½“å’Œå…³ç³»æŠ½å–
            extraction_config = extraction_config or {}
            entity_model = extraction_config.get("entity_model", "bert-ner")
            relation_model = extraction_config.get("relation_model", "bert-relation")
            confidence_threshold = extraction_config.get("confidence_threshold", 0.7)
            
            tasks = [
                self.extract_entities_from_text(
                    document_content, user_id, entity_model, confidence_threshold
                ),
                self.extract_relations_from_text(
                    document_content, user_id, relation_model, confidence_threshold
                )
            ]
            
            entities, relations = await asyncio.gather(*tasks)
            
            # 3. å®ä½“é“¾æ¥å’Œå»é‡
            entities = await self._process_entities(entities, user_id)
            relations = await self._process_relations(relations, entities, user_id)
            
            # 4. æ„å»ºå›¾è°±ç»“æ„
            graph_structure = await self._build_graph_structure(entities, relations)
            
            # 5. ä¿å­˜åˆ°å›¾æ•°æ®åº“
            graph_result = await self._save_graph_to_database(
                document_id, user_id, graph_structure, document
            )
            
            if not graph_result.get("success"):
                return graph_result
            
            graph_id = graph_result["graph_id"]
            
            # 6. ç”Ÿæˆå›¾è°±æ‘˜è¦
            graph_summary = await self._generate_graph_summary(graph_structure)
            
            # 7. å‘å¸ƒå®Œæˆäº‹ä»¶
            processing_time = (datetime.now() - start_time).total_seconds()
            await publish_event(
                "knowledge_graph.built",
                {
                    "document_id": document_id,
                    "graph_id": graph_id,
                    "user_id": user_id,
                    "entity_count": len(entities),
                    "relation_count": len(relations),
                    "processing_time": processing_time,
                    "timestamp": datetime.now().isoformat()
                }
            )
            
            # æ›´æ–°ç»Ÿè®¡
            self.processing_stats["total_documents"] += 1
            self.processing_stats["processing_time"] += processing_time
            
            logger.info(f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼Œå›¾è°±ID: {graph_id}, è€—æ—¶: {processing_time:.2f}ç§’")
            
            return {
                "success": True,
                "graph_id": graph_id,
                "entity_count": len(entities),
                "relation_count": len(relations),
                "processing_time": processing_time,
                "graph_summary": graph_summary,
                "status": "completed"
            }
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "document_id": document_id
            }
    
    async def extract_relations_from_text(
        self, 
        text: str, 
        user_id: str,
        model: str = "bert-relation",
        confidence_threshold: float = 0.6
    ) -> List[Dict[str, Any]]:
        """ä»æ–‡æœ¬ä¸­æå–å…³ç³»ï¼ˆè°ƒç”¨æ¨¡å‹æœåŠ¡ï¼‰"""
        try:
            result = await self.service_client.call(
                service_name="model-service",
                method=CallMethod.POST,
                path="/api/v1/nlp/relation-extraction",
                config=self.model_config,
                json={
                    "text": text,
                    "model": model,
                    "user_id": user_id,
                    "confidence_threshold": confidence_threshold
                }
            )
            
            raw_relations = result.get("relations", [])
            
            # æ ‡å‡†åŒ–å…³ç³»æ ¼å¼
            relations = []
            for relation in raw_relations:
                if relation.get("confidence", 0) >= confidence_threshold:
                    standardized = self.graph_processor.standardize_relation(relation)
                    relations.append(standardized)
            
            # æ›´æ–°ç»Ÿè®¡
            self.processing_stats["total_relations"] += len(relations)
            
            logger.info(f"æå–åˆ° {len(relations)} ä¸ªå…³ç³» (é˜ˆå€¼: {confidence_threshold})")
            
            return relations
            
        except ServiceCallError as e:
            logger.error(f"å…³ç³»æå–å¤±è´¥: {e}")
            if e.status_code == 503:
                # æœåŠ¡ä¸å¯ç”¨ï¼Œè¿”å›ç©ºåˆ—è¡¨
                return []
            raise
    
    async def _fallback_entity_extraction(self, text: str) -> List[Dict[str, Any]]:
        """å®ä½“æå–çš„é™çº§ç­–ç•¥"""
        # ç®€å•çš„è§„åˆ™æå–ï¼ˆä½œä¸ºé™çº§ç­–ç•¥ï¼‰
        import re
        
        entities = []
        
        # äººåæå–
        person_pattern = r'[\u4e00-\u9fa5]{2,4}(?:å…ˆç”Ÿ|å¥³å£«|æ•™æˆ|åšå£«|è€å¸ˆ|ä¸»ä»»|ç»ç†|æ€»ç›‘|è‘£äº‹é•¿)?'
        persons = re.findall(person_pattern, text)
        for person in set(persons)[:5]:  # å»é‡å¹¶é™åˆ¶æ•°é‡
            entities.append({
                "id": str(uuid.uuid4()),
                "text": person,
                "type": "PERSON",
                "confidence": 0.5,
                "properties": {"fallback": True}
            })
        
        # ç»„ç»‡æœºæ„æå–
        org_pattern = r'[\u4e00-\u9fa5]{2,10}(?:å…¬å¸|ä¼ä¸š|é›†å›¢|æœºæ„|ç»„ç»‡|éƒ¨é—¨|å­¦é™¢|å¤§å­¦|ç ”ç©¶æ‰€)'
        orgs = re.findall(org_pattern, text)
        for org in set(orgs)[:5]:
            entities.append({
                "id": str(uuid.uuid4()),
                "text": org,
                "type": "ORGANIZATION", 
                "confidence": 0.4,
                "properties": {"fallback": True}
            })
        
        # åœ°ç‚¹æå–
        location_pattern = r'[\u4e00-\u9fa5]{2,8}(?:çœ|å¸‚|å¿|åŒº|é•‡|æ‘|è¡—é“|è·¯|å¤§é“|å¹¿åœº|åŒ»é™¢|å­¦æ ¡)'
        locations = re.findall(location_pattern, text)
        for location in set(locations)[:3]:
            entities.append({
                "id": str(uuid.uuid4()),
                "text": location,
                "type": "LOCATION",
                "confidence": 0.3,
                "properties": {"fallback": True}
            })
        
        logger.info(f"é™çº§ç­–ç•¥æå–åˆ° {len(entities)} ä¸ªå®ä½“")
        return entities
    
    # ==================== æƒé™å’Œè®¤è¯ ====================
    
    async def _check_graph_permission(self, user_id: str, operation: str) -> Dict[str, Any]:
        """æ£€æŸ¥çŸ¥è¯†å›¾è°±æ“ä½œæƒé™"""
        try:
            result = await self.service_client.call(
                service_name="base-service",
                method=CallMethod.POST,
                path="/api/v1/auth/check-permission",
                config=self.auth_config,
                json={
                    "user_id": user_id,
                    "resource_type": "KNOWLEDGE_GRAPH",
                    "action": operation,
                    "context": {
                        "service": "knowledge-graph-service",
                        "operation": operation
                    }
                }
            )
            
            return result
            
        except Exception as e:
            logger.error(f"æƒé™æ£€æŸ¥å¤±è´¥: {e}")
            return {"allowed": False, "error": str(e)}
    
    # ==================== æ•°æ®è·å–å’Œå¤„ç† ====================
    
    async def _get_document_content(self, document_id: str, user_id: str) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£å†…å®¹"""
        try:
            result = await self.service_client.call(
                service_name="knowledge-service",
                method=CallMethod.GET,
                path=f"/api/v1/documents/{document_id}",
                config=self.knowledge_config,
                params={"user_id": user_id}
            )
            
            return {
                "success": True,
                "data": result
            }
            
        except ServiceCallError as e:
            logger.error(f"è·å–æ–‡æ¡£å¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"æ–‡æ¡£ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®: {document_id}",
                "status_code": e.status_code
            }
    
    async def _process_entities(self, entities: List[Dict[str, Any]], user_id: str) -> List[Dict[str, Any]]:
        """å¤„ç†å®ä½“ï¼šå»é‡ã€é“¾æ¥ã€æ ‡å‡†åŒ–"""
        if not entities:
            return []
        
        # å®ä½“å»é‡ (åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦)
        unique_entities = []
        seen_texts = set()
        
        for entity in entities:
            entity_text = entity.get("text", "").lower().strip()
            if entity_text and entity_text not in seen_texts:
                seen_texts.add(entity_text)
                unique_entities.append(entity)
        
        # å®ä½“é“¾æ¥ (å¯ä»¥è°ƒç”¨å¤–éƒ¨çŸ¥è¯†åº“APIè¿›è¡Œå®ä½“é“¾æ¥)
        linked_entities = []
        for entity in unique_entities:
            # è¿™é‡Œå¯ä»¥æ‰©å±•å®ä½“é“¾æ¥é€»è¾‘
            entity["linked"] = False
            entity["kb_id"] = None
            linked_entities.append(entity)
        
        logger.info(f"å®ä½“å¤„ç†å®Œæˆï¼š{len(entities)} -> {len(linked_entities)}")
        return linked_entities
    
    async def _process_relations(
        self, 
        relations: List[Dict[str, Any]], 
        entities: List[Dict[str, Any]], 
        user_id: str
    ) -> List[Dict[str, Any]]:
        """å¤„ç†å…³ç³»ï¼šéªŒè¯ã€è¿‡æ»¤ã€æ ‡å‡†åŒ–"""
        if not relations:
            return []
        
        entity_texts = {entity.get("text", "").lower() for entity in entities}
        
        valid_relations = []
        for relation in relations:
            source = relation.get("source", "").lower()
            target = relation.get("target", "").lower()
            
            # éªŒè¯å…³ç³»çš„å®ä½“æ˜¯å¦å­˜åœ¨
            if source in entity_texts and target in entity_texts:
                valid_relations.append(relation)
        
        logger.info(f"å…³ç³»å¤„ç†å®Œæˆï¼š{len(relations)} -> {len(valid_relations)}")
        return valid_relations
    
    async def _build_graph_structure(self, entities: List[Dict[str, Any]], relations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ„å»ºå›¾è°±ç»“æ„"""
        # æ„å»ºèŠ‚ç‚¹å’Œè¾¹çš„å›¾ç»“æ„
        nodes = []
        edges = []
        
        entity_map = {}
        for entity in entities:
            node_id = entity.get("id", str(uuid.uuid4()))
            entity_map[entity.get("text", "")] = node_id
            
            nodes.append({
                "id": node_id,
                "label": entity.get("text", ""),
                "type": entity.get("type", "CONCEPT"),
                "properties": entity.get("properties", {}),
                "confidence": entity.get("confidence", 0.8)
            })
        
        for relation in relations:
            source_text = relation.get("source", "")
            target_text = relation.get("target", "")
            
            if source_text in entity_map and target_text in entity_map:
                edges.append({
                    "id": relation.get("id", str(uuid.uuid4())),
                    "source": entity_map[source_text],
                    "target": entity_map[target_text],
                    "type": relation.get("type", "RELATED_TO"),
                    "properties": relation.get("properties", {}),
                    "confidence": relation.get("confidence", 0.7)
                })
        
        return {
            "nodes": nodes,
            "edges": edges,
            "node_count": len(nodes),
            "edge_count": len(edges),
            "created_at": datetime.now().isoformat()
        }
    
    async def _save_graph_to_database(
        self, 
        document_id: str, 
        user_id: str, 
        graph_structure: Dict[str, Any],
        document_metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ä¿å­˜å›¾è°±åˆ°æ•°æ®åº“"""
        try:
            result = await self.service_client.call(
                service_name="database-service",
                method=CallMethod.POST,
                path="/api/v1/graph/create",
                config=self.database_config,
                json={
                    "document_id": document_id,
                    "user_id": user_id,
                    "graph_data": graph_structure,
                    "metadata": {
                        "document_title": document_metadata.get("title", ""),
                        "document_type": document_metadata.get("type", ""),
                        "created_at": datetime.now().isoformat(),
                        "extraction_stats": {
                            "node_count": graph_structure["node_count"],
                            "edge_count": graph_structure["edge_count"]
                        }
                    }
                }
            )
            
            return {
                "success": True,
                "graph_id": result.get("graph_id")
            }
            
        except Exception as e:
            logger.error(f"ä¿å­˜å›¾è°±å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _generate_graph_summary(self, graph_structure: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå›¾è°±æ‘˜è¦"""
        nodes = graph_structure.get("nodes", [])
        edges = graph_structure.get("edges", [])
        
        # ç»Ÿè®¡å®ä½“ç±»å‹
        entity_types = defaultdict(int)
        for node in nodes:
            entity_types[node.get("type", "UNKNOWN")] += 1
        
        # ç»Ÿè®¡å…³ç³»ç±»å‹
        relation_types = defaultdict(int)
        for edge in edges:
            relation_types[edge.get("type", "UNKNOWN")] += 1
        
        # æ‰¾å‡ºåº¦æœ€é«˜çš„èŠ‚ç‚¹
        node_degrees = defaultdict(int)
        for edge in edges:
            node_degrees[edge.get("source", "")] += 1
            node_degrees[edge.get("target", "")] += 1
        
        top_nodes = sorted(node_degrees.items(), key=lambda x: x[1], reverse=True)[:5]
        
        return {
            "total_entities": len(nodes),
            "total_relations": len(edges),
            "entity_types": dict(entity_types),
            "relation_types": dict(relation_types),
            "top_connected_entities": top_nodes,
            "graph_density": len(edges) / max(len(nodes) * (len(nodes) - 1) / 2, 1) if len(nodes) > 1 else 0
        }


    # ==================== æ‰¹é‡å¤„ç†å’ŒæŸ¥è¯¢ ====================
    
    async def batch_process_documents(self, document_ids: List[str], user_id: str) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æ–‡æ¡£ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±"""
        start_time = datetime.now()
        results = {
            "success": 0,
            "failed": 0,
            "total": len(document_ids),
            "results": [],
            "failed_documents": []
        }
        
        logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {len(document_ids)} ä¸ªæ–‡æ¡£")
        
        # é™åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…è¿‡è½½
        semaphore = asyncio.Semaphore(3)
        
        async def process_single_document(doc_id: str) -> Dict[str, Any]:
            async with semaphore:
                try:
                    result = await self.build_knowledge_graph_workflow(doc_id, user_id)
                    if result.get("success"):
                        results["success"] += 1
                        results["results"].append({
                            "document_id": doc_id,
                            "graph_id": result.get("graph_id"),
                            "entity_count": result.get("entity_count"),
                            "relation_count": result.get("relation_count")
                        })
                    else:
                        results["failed"] += 1
                        results["failed_documents"].append({
                            "document_id": doc_id,
                            "error": result.get("error")
                        })
                    return result
                except Exception as e:
                    results["failed"] += 1
                    results["failed_documents"].append({
                        "document_id": doc_id,
                        "error": str(e)
                    })
                    logger.error(f"å¤„ç†æ–‡æ¡£ {doc_id} å¤±è´¥: {e}")
                    return {"success": False, "error": str(e)}
        
        # å¹¶å‘å¤„ç†
        await asyncio.gather(
            *[process_single_document(doc_id) for doc_id in document_ids],
            return_exceptions=True
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        results["processing_time"] = processing_time
        
        logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆï¼šæˆåŠŸ {results['success']}/{results['total']}, è€—æ—¶: {processing_time:.2f}ç§’")
        
        return results
    
    async def query_related_knowledge(
        self, 
        query: str, 
        user_id: str,
        limit: int = 10,
        confidence_threshold: float = 0.5
    ) -> Dict[str, Any]:
        """æŸ¥è¯¢ç›¸å…³çŸ¥è¯†"""
        try:
            # æƒé™æ£€æŸ¥
            permission_check = await self._check_graph_permission(user_id, "query")
            if not permission_check.get("allowed"):
                return {
                    "success": False,
                    "error": "æƒé™ä¸è¶³",
                    "required_permission": "knowledge_graph:query"
                }
            
            # è°ƒç”¨å›¾æ•°æ®åº“æŸ¥è¯¢ç›¸å…³å®ä½“å’Œå…³ç³»
            result = await self.service_client.call(
                service_name="database-service",
                method=CallMethod.POST,
                path="/api/v1/graph/query",
                config=self.database_config,
                json={
                    "query": query,
                    "user_id": user_id,
                    "query_type": "semantic_search",
                    "limit": limit,
                    "confidence_threshold": confidence_threshold
                }
            )
            
            return {
                "success": True,
                "query": query,
                "results": result.get("results", []),
                "total_count": result.get("total_count", 0)
            }
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "query": query
            }
    
    async def graph_reasoning(
        self, 
        source_entity: str, 
        target_entity: str, 
        user_id: str,
        max_depth: int = 3,
        reasoning_type: str = "shortest_path"
    ) -> Dict[str, Any]:
        """å›¾è°±æ¨ç†ï¼šæŸ¥æ‰¾å®ä½“é—´çš„è·¯å¾„"""
        try:
            # æƒé™æ£€æŸ¥
            permission_check = await self._check_graph_permission(user_id, "reasoning")
            if not permission_check.get("allowed"):
                return {
                    "success": False,
                    "error": "æƒé™ä¸è¶³"
                }
            
            result = await self.service_client.call(
                service_name="database-service",
                method=CallMethod.POST,
                path="/api/v1/graph/reasoning",
                config=self.database_config,
                json={
                    "source_entity": source_entity,
                    "target_entity": target_entity,
                    "user_id": user_id,
                    "reasoning_type": reasoning_type,
                    "max_depth": max_depth
                }
            )
            
            return {
                "success": True,
                "source": source_entity,
                "target": target_entity,
                "paths": result.get("paths", []),
                "shortest_distance": result.get("shortest_distance", -1)
            }
            
        except Exception as e:
            logger.error(f"å›¾è°±æ¨ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    # ==================== æœåŠ¡ç›‘æ§å’Œç®¡ç† ====================
    
    async def health_check_all_services(self) -> Dict[str, bool]:
        """æ£€æŸ¥æ‰€æœ‰ç›¸å…³æœåŠ¡çš„å¥åº·çŠ¶æ€"""
        services = ["model-service", "knowledge-service", "database-service", "base-service"]
        health_status = {}
        
        async def check_service_health(service_name: str) -> Tuple[str, bool]:
            try:
                result = await self.service_client.call(
                    service_name=service_name,
                    method=CallMethod.GET,
                    path="/health",
                    config=CallConfig(timeout=5, retry_times=1)
                )
                return service_name, result.get("status") == "healthy"
            except Exception:
                return service_name, False
        
        # å¹¶å‘æ£€æŸ¥æ‰€æœ‰æœåŠ¡
        health_results = await asyncio.gather(
            *[check_service_health(service) for service in services],
            return_exceptions=True
        )
        
        for service_name, is_healthy in health_results:
            if isinstance(is_healthy, bool):
                health_status[service_name] = is_healthy
            else:
                health_status[service_name] = False
        
        return health_status
    
    async def get_service_metrics(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡è°ƒç”¨æŒ‡æ ‡"""
        return {
            "processing_stats": self.processing_stats.copy(),
            "graph_capabilities": self.graph_capabilities,
            "service_configs": {
                "model_timeout": self.model_config.timeout,
                "knowledge_timeout": self.knowledge_config.timeout,
                "database_timeout": self.database_config.timeout,
                "auth_timeout": self.auth_config.timeout
            },
            "last_updated": datetime.now().isoformat()
        }
    
    async def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.processing_stats = {
            "total_documents": 0,
            "total_entities": 0,
            "total_relations": 0,
            "processing_time": 0.0
        }
        logger.info("ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")


# ==================== ä¾¿æ·çš„å…¨å±€å‡½æ•° ====================

async def build_knowledge_graph(document_id: str, user_id: str, extraction_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """ä¾¿æ·çš„çŸ¥è¯†å›¾è°±æ„å»ºå‡½æ•°"""
    async with KnowledgeGraphServiceIntegration() as kg_service:
        return await kg_service.build_knowledge_graph_workflow(document_id, user_id, extraction_config)


async def batch_build_knowledge_graphs(document_ids: List[str], user_id: str) -> Dict[str, Any]:
    """ä¾¿æ·çš„æ‰¹é‡çŸ¥è¯†å›¾è°±æ„å»ºå‡½æ•°"""
    async with KnowledgeGraphServiceIntegration() as kg_service:
        return await kg_service.batch_process_documents(document_ids, user_id)


async def query_knowledge_graph(query: str, user_id: str, limit: int = 10) -> Dict[str, Any]:
    """ä¾¿æ·çš„çŸ¥è¯†å›¾è°±æŸ¥è¯¢å‡½æ•°"""
    async with KnowledgeGraphServiceIntegration() as kg_service:
        return await kg_service.query_related_knowledge(query, user_id, limit)


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

async def knowledge_graph_demo():
    """çŸ¥è¯†å›¾è°±æœåŠ¡é›†æˆæ¨¡å—"""
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    async with KnowledgeGraphServiceIntegration() as kg_service:
        
        # 1. æœåŠ¡å¥åº·æ£€æŸ¥
        logger.info("=== ğŸ¥ æœåŠ¡å¥åº·æ£€æŸ¥ ===")
        health_status = await kg_service.health_check_all_services()
        for service, status in health_status.items():
            print(f"{service}: {'âœ… æ­£å¸¸' if status else 'âŒ å¼‚å¸¸'}")
        
        # 2. å•æ–‡æ¡£çŸ¥è¯†å›¾è°±æ„å»º
        logger.info("\n=== ğŸ“„ å•æ–‡æ¡£å›¾è°±æ„å»º ===")
        extraction_config = {
            "entity_model": "bert-ner",
            "relation_model": "bert-relation", 
            "confidence_threshold": 0.7
        }
        
        try:
            result = await kg_service.build_knowledge_graph_workflow(
                "doc_001", "user_123", extraction_config
            )
            if result.get("success"):
                print(f"âœ… å›¾è°±æ„å»ºæˆåŠŸ:")
                print(f"   å›¾è°±ID: {result.get('graph_id')}")
                print(f"   å®ä½“æ•°: {result.get('entity_count')}")
                print(f"   å…³ç³»æ•°: {result.get('relation_count')}")
                print(f"   å¤„ç†æ—¶é—´: {result.get('processing_time'):.2f}ç§’")
            else:
                print(f"âŒ å›¾è°±æ„å»ºå¤±è´¥: {result.get('error')}")
        except Exception as e:
            print(f"âŒ å¼‚å¸¸: {e}")
        
        # 3. æ‰¹é‡æ–‡æ¡£å¤„ç†
        logger.info("\n=== ğŸ“š æ‰¹é‡æ–‡æ¡£å¤„ç† ===")
        document_ids = ["doc_001", "doc_002", "doc_003", "doc_004"]
        batch_result = await kg_service.batch_process_documents(document_ids, "user_123")
        
        print(f"æ‰¹é‡å¤„ç†ç»“æœ:")
        print(f"   æ€»æ–‡æ¡£æ•°: {batch_result['total']}")
        print(f"   æˆåŠŸ: {batch_result['success']}")
        print(f"   å¤±è´¥: {batch_result['failed']}")
        print(f"   å¤„ç†æ—¶é—´: {batch_result.get('processing_time', 0):.2f}ç§’")
        
        if batch_result.get('failed_documents'):
            print(f"   å¤±è´¥æ–‡æ¡£: {[doc['document_id'] for doc in batch_result['failed_documents']]}")
        
        # 4. å®ä½“æŠ½å–æµ‹è¯•
        logger.info("\n=== ğŸ” å®ä½“æŠ½å–æµ‹è¯• ===")
        test_text = "å¼ ä¸‰æ˜¯åŒ—äº¬å¤§å­¦çš„æ•™æˆï¼Œä»–åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸæœ‰å¾ˆæ·±çš„ç ”ç©¶ã€‚"
        entities = await kg_service.extract_entities_from_text(test_text, "user_123")
        print(f"ä»æ–‡æœ¬ä¸­æå–åˆ° {len(entities)} ä¸ªå®ä½“:")
        for entity in entities[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   - {entity.get('text')} ({entity.get('type')}, ç½®ä¿¡åº¦: {entity.get('confidence'):.2f})")
        
        # 5. å…³ç³»æŠ½å–æµ‹è¯•
        logger.info("\n=== ğŸ”— å…³ç³»æŠ½å–æµ‹è¯• ===")
        relations = await kg_service.extract_relations_from_text(test_text, "user_123")
        print(f"ä»æ–‡æœ¬ä¸­æå–åˆ° {len(relations)} ä¸ªå…³ç³»:")
        for relation in relations[:5]:
            print(f"   - {relation.get('source')} â†’ {relation.get('target')} ({relation.get('type')})")
        
        # 6. çŸ¥è¯†æŸ¥è¯¢
        logger.info("\n=== ğŸ” çŸ¥è¯†æŸ¥è¯¢ ===")
        query_result = await kg_service.query_related_knowledge("äººå·¥æ™ºèƒ½", "user_123", limit=5)
        if query_result.get("success"):
            print(f"æŸ¥è¯¢ 'äººå·¥æ™ºèƒ½' ç›¸å…³çŸ¥è¯†:")
            print(f"   æ‰¾åˆ° {query_result.get('total_count', 0)} æ¡ç›¸å…³ç»“æœ")
            for i, result in enumerate(query_result.get('results', [])[:3], 1):
                print(f"   {i}. {result}")
        else:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {query_result.get('error')}")
        
        # 7. å›¾è°±æ¨ç†
        logger.info("\n=== ğŸ§  å›¾è°±æ¨ç† ===")
        reasoning_result = await kg_service.graph_reasoning(
            "å¼ ä¸‰", "åŒ—äº¬å¤§å­¦", "user_123", max_depth=3
        )
        if reasoning_result.get("success"):
            print(f"æ¨ç†è·¯å¾„ 'å¼ ä¸‰' â†’ 'åŒ—äº¬å¤§å­¦':")
            paths = reasoning_result.get("paths", [])
            if paths:
                print(f"   æ‰¾åˆ° {len(paths)} æ¡è·¯å¾„")
                print(f"   æœ€çŸ­è·ç¦»: {reasoning_result.get('shortest_distance')}")
            else:
                print("   æœªæ‰¾åˆ°è¿æ¥è·¯å¾„")
        else:
            print(f"âŒ æ¨ç†å¤±è´¥: {reasoning_result.get('error')}")
        
        # 8. æœåŠ¡æŒ‡æ ‡
        logger.info("\n=== ğŸ“Š æœåŠ¡æŒ‡æ ‡ ===")
        metrics = await kg_service.get_service_metrics()
        stats = metrics.get("processing_stats", {})
        print(f"å¤„ç†ç»Ÿè®¡:")
        print(f"   å·²å¤„ç†æ–‡æ¡£: {stats.get('total_documents', 0)}")
        print(f"   æŠ½å–å®ä½“: {stats.get('total_entities', 0)}")
        print(f"   æŠ½å–å…³ç³»: {stats.get('total_relations', 0)}")
        print(f"   æ€»å¤„ç†æ—¶é—´: {stats.get('processing_time', 0):.2f}ç§’")
        
        # 9. é‡ç½®ç»Ÿè®¡
        await kg_service.reset_stats()
        logger.info("âœ… ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")


# ç®€å•çš„å•ç”¨é€”å‡½æ•°ç¤ºä¾‹
async def simple_examples():
    """ç®€å•ä½¿ç”¨ç¤ºä¾‹"""
    
    # æ„å»ºå•ä¸ªæ–‡æ¡£çš„çŸ¥è¯†å›¾è°±
    result = await build_knowledge_graph("doc_001", "user_123")
    print(f"å›¾è°±æ„å»ºç»“æœ: {result}")
    
    # æ‰¹é‡æ„å»ºçŸ¥è¯†å›¾è°±
    batch_result = await batch_build_knowledge_graphs(
        ["doc_001", "doc_002"], "user_123"
    )
    print(f"æ‰¹é‡å¤„ç†ç»“æœ: {batch_result}")
    
    # æŸ¥è¯¢çŸ¥è¯†å›¾è°±
    query_result = await query_knowledge_graph("æœºå™¨å­¦ä¹ ", "user_123")
    print(f"æŸ¥è¯¢ç»“æœ: {query_result}")


if __name__ == "__main__":
    print("ğŸš€ çŸ¥è¯†å›¾è°±æœåŠ¡é›†æˆå¯åŠ¨")
    print("=" * 50)
    asyncio.run(knowledge_graph_demo()) 