# 工具服务集成方案

## 项目概述

本方案旨在为ZZDSJ智政科技AI智能办公助手集成三个开源工具框架：
- **WebAgent**: 阿里巴巴开源的Web搜索Agent
- **Crawlee**: Apify开源的Node.js自动化爬虫框架
- **Scraperr**: 自托管的网页爬虫项目

集成目标是为Agno框架的Agent和LlamaIndex知识库提供实时搜索和数据爬取能力。

## 技术架构分析

### 1. WebAgent (Alibaba-NLP)
**技术特点**:
- 基于Python的Web搜索智能体
- 支持多种搜索引擎集成
- 提供智能搜索策略和结果筛选
- 适合集成到现有Python微服务架构

**集成优势**:
- 与当前Python技术栈完全兼容
- 可直接集成到Agno Agent工具链
- 提供智能化的搜索结果处理

### 2. Crawlee (Apify)
**技术特点**:
- Node.js/TypeScript实现
- 支持HTTP和浏览器爬虫
- 内置反检测机制
- 强大的代理和会话管理

**集成挑战**:
- 需要Node.js运行环境
- 跨语言调用复杂度
- 需要独立部署和管理

### 3. Scraperr (自托管)
**技术特点**:
- FastAPI后端 + Next.js前端
- 无代码可视化配置
- 支持队列管理和任务调度
- MongoDB数据存储

**集成优势**:
- FastAPI与当前技术栈一致
- 提供管理界面
- 支持RESTful API调用

## 集成方案设计

### 方案一: 统一工具微服务架构 (推荐)

#### 1.1 架构设计
```
┌─────────────────────────────────────────────────────────┐
│                    工具微服务 (tools-service)                    │
├─────────────────────────────────────────────────────────┤
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │
│  │   搜索工具模块      │  │   爬虫工具模块      │  │   数据处理模块      │  │
│  │  (WebAgent)    │  │   (多引擎)       │  │   (清洗/转换)      │  │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘  │
│  ┌─────────────────────────────────────────────────────────┐  │
│  │              统一工具接口层 (Tool Interface)               │  │
│  └─────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
                              │
        ┌─────────────────────┼─────────────────────┐
        │                     │                     │
   ┌─────────┐          ┌─────────┐          ┌─────────┐
   │  Agent  │          │ Knowledge│          │  Other  │
   │ Service │          │ Service  │          │Services │
   │ (Agno)  │          │(LlamaIdx)│          │         │
   └─────────┘          └─────────┘          └─────────┘
```

#### 1.2 技术实现
- **核心语言**: Python (FastAPI)
- **WebAgent**: 直接集成Python库
- **Crawlee**: 通过subprocess调用Node.js进程
- **Scraperr**: 集成核心爬虫逻辑，复用FastAPI架构

#### 1.3 工具接口标准
```python
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from pydantic import BaseModel

class ToolRequest(BaseModel):
    tool_name: str
    action: str
    parameters: Dict[str, Any]
    context: Optional[Dict[str, Any]] = None

class ToolResponse(BaseModel):
    success: bool
    data: Any
    message: str
    metadata: Optional[Dict[str, Any]] = None

class BaseTool(ABC):
    @abstractmethod
    async def execute(self, request: ToolRequest) -> ToolResponse:
        pass
    
    @abstractmethod
    def get_schema(self) -> Dict[str, Any]:
        pass
```

### 方案二: 分布式工具服务架构

#### 2.1 架构设计
```
┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐
│  WebAgent       │  │   Crawlee       │  │   Scraperr      │
│  Service        │  │   Service       │  │   Service       │
│  (Python)       │  │   (Node.js)     │  │   (Python)      │
└─────────────────┘  └─────────────────┘  └─────────────────┘
         │                    │                    │
         └──────────────────┬─┴─────────────────────┘
                            │
┌─────────────────────────────────────────────────────────┐
│              工具网关服务 (Tool Gateway)                    │
│           负载均衡 + 路由 + 统一接口                           │
└─────────────────────────────────────────────────────────┘
```

## Agno框架工具集成

### 3.1 Agno工具声明方式
```python
from agno import Agent, tool
from typing import Dict, Any

class SearchAndCrawlAgent(Agent):
    def __init__(self):
        super().__init__(name="搜索爬虫智能体")
        
    @tool(name="web_search")
    async def web_search(self, query: str, engine: str = "google") -> Dict[str, Any]:
        """智能网页搜索工具"""
        request = ToolRequest(
            tool_name="webagent",
            action="search",
            parameters={"query": query, "engine": engine}
        )
        result = await self.tools_client.call_tool(request)
        return result.data
    
    @tool(name="web_crawl")
    async def web_crawl(self, url: str, extract_rules: Dict[str, Any]) -> Dict[str, Any]:
        """网页数据爬取工具"""
        request = ToolRequest(
            tool_name="crawlee",
            action="crawl",
            parameters={"url": url, "rules": extract_rules}
        )
        result = await self.tools_client.call_tool(request)
        return result.data
    
    @tool(name="site_spider")
    async def site_spider(self, domain: str, max_pages: int = 100) -> Dict[str, Any]:
        """站点爬虫工具"""
        request = ToolRequest(
            tool_name="scraperr",
            action="spider",
            parameters={"domain": domain, "max_pages": max_pages}
        )
        result = await self.tools_client.call_tool(request)
        return result.data
```

### 3.2 LlamaIndex自定义工具集成
```python
from llama_index.core.tools import BaseTool, ToolMetadata
from llama_index.core.bridge.pydantic import BaseModel, Field

class WebSearchTool(BaseTool):
    metadata = ToolMetadata(
        name="web_search",
        description="智能网页搜索工具，支持多搜索引擎",
        fn_schema=WebSearchSchema
    )
    
    def __init__(self, tools_client):
        self.tools_client = tools_client
        
    async def acall(self, query: str, engine: str = "google") -> str:
        request = ToolRequest(
            tool_name="webagent",
            action="search", 
            parameters={"query": query, "engine": engine}
        )
        result = await self.tools_client.call_tool(request)
        return str(result.data)

class WebCrawlTool(BaseTool):
    metadata = ToolMetadata(
        name="web_crawl",
        description="网页数据爬取工具",
        fn_schema=WebCrawlSchema
    )
    
    async def acall(self, url: str, xpath: str = None) -> str:
        request = ToolRequest(
            tool_name="crawlee",
            action="crawl",
            parameters={"url": url, "xpath": xpath}
        )
        result = await self.tools_client.call_tool(request)
        return str(result.data)
```

## 工具微服务详细设计

### 4.1 服务架构
```python
# tools-service/main.py
from fastapi import FastAPI, HTTPException
from .tools import WebAgentTool, CrawleeTool, ScraperTool
from .schemas import ToolRequest, ToolResponse

app = FastAPI(title="工具微服务", version="1.0.0")

# 工具注册表
TOOLS_REGISTRY = {
    "webagent": WebAgentTool(),
    "crawlee": CrawleeTool(), 
    "scraperr": ScraperTool()
}

@app.post("/api/v1/tools/execute", response_model=ToolResponse)
async def execute_tool(request: ToolRequest):
    """执行工具调用"""
    if request.tool_name not in TOOLS_REGISTRY:
        raise HTTPException(status_code=404, detail=f"工具 {request.tool_name} 未找到")
    
    tool = TOOLS_REGISTRY[request.tool_name]
    try:
        result = await tool.execute(request)
        return result
    except Exception as e:
        return ToolResponse(
            success=False,
            data=None,
            message=f"工具执行失败: {str(e)}"
        )

@app.get("/api/v1/tools/list")
async def list_tools():
    """获取所有可用工具"""
    tools = []
    for name, tool in TOOLS_REGISTRY.items():
        tools.append({
            "name": name,
            "schema": tool.get_schema(),
            "status": "active"
        })
    return {"tools": tools}
```

### 4.2 WebAgent工具实现
```python
# tools-service/tools/webagent_tool.py
import asyncio
from typing import Dict, Any
from ..base import BaseTool, ToolRequest, ToolResponse

class WebAgentTool(BaseTool):
    def __init__(self):
        self.name = "webagent"
        # 初始化WebAgent实例
        
    async def execute(self, request: ToolRequest) -> ToolResponse:
        action = request.action
        params = request.parameters
        
        try:
            if action == "search":
                result = await self._search(
                    query=params.get("query"),
                    engine=params.get("engine", "google"),
                    max_results=params.get("max_results", 10)
                )
            elif action == "analyze":
                result = await self._analyze_results(
                    results=params.get("results"),
                    criteria=params.get("criteria")
                )
            else:
                raise ValueError(f"不支持的操作: {action}")
                
            return ToolResponse(
                success=True,
                data=result,
                message="执行成功"
            )
        except Exception as e:
            return ToolResponse(
                success=False,
                data=None,
                message=str(e)
            )
    
    async def _search(self, query: str, engine: str, max_results: int) -> Dict[str, Any]:
        # WebAgent搜索逻辑
        pass
    
    def get_schema(self) -> Dict[str, Any]:
        return {
            "name": "webagent",
            "description": "智能网页搜索工具",
            "actions": {
                "search": {
                    "description": "执行智能搜索",
                    "parameters": {
                        "query": {"type": "string", "required": True},
                        "engine": {"type": "string", "default": "google"},
                        "max_results": {"type": "integer", "default": 10}
                    }
                }
            }
        }
```

### 4.3 Crawlee工具实现
```python
# tools-service/tools/crawlee_tool.py
import subprocess
import json
import tempfile
from pathlib import Path

class CrawleeTool(BaseTool):
    def __init__(self):
        self.name = "crawlee"
        self.node_script_path = Path(__file__).parent / "crawlee_wrapper.js"
        
    async def execute(self, request: ToolRequest) -> ToolResponse:
        action = request.action
        params = request.parameters
        
        try:
            if action == "crawl":
                result = await self._crawl_page(
                    url=params.get("url"),
                    rules=params.get("rules", {}),
                    options=params.get("options", {})
                )
            elif action == "bulk_crawl":
                result = await self._bulk_crawl(
                    urls=params.get("urls"),
                    rules=params.get("rules", {})
                )
            else:
                raise ValueError(f"不支持的操作: {action}")
                
            return ToolResponse(
                success=True,
                data=result,
                message="爬取成功"
            )
        except Exception as e:
            return ToolResponse(
                success=False,
                data=None,
                message=str(e)
            )
    
    async def _crawl_page(self, url: str, rules: Dict, options: Dict) -> Dict[str, Any]:
        # 创建临时配置文件
        config = {
            "url": url,
            "rules": rules,
            "options": options
        }
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(config, f)
            config_path = f.name
        
        try:
            # 调用Node.js脚本
            result = subprocess.run([
                'node', str(self.node_script_path), config_path
            ], capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                return json.loads(result.stdout)
            else:
                raise Exception(f"Crawlee执行失败: {result.stderr}")
        finally:
            Path(config_path).unlink()  # 清理临时文件
```

### 4.4 Node.js包装脚本
```javascript
// tools-service/tools/crawlee_wrapper.js
const { PlaywrightCrawler, Dataset } = require('crawlee');
const fs = require('fs');

async function main() {
    const configPath = process.argv[2];
    const config = JSON.parse(fs.readFileSync(configPath, 'utf8'));
    
    const results = [];
    
    const crawler = new PlaywrightCrawler({
        maxRequestsPerCrawl: config.options.maxPages || 10,
        async requestHandler({ request, page, enqueueLinks, log }) {
            try {
                // 根据规则提取数据
                const data = await extractData(page, config.rules);
                results.push({
                    url: request.loadedUrl,
                    data: data,
                    timestamp: new Date().toISOString()
                });
                
                // 如果需要爬取链接
                if (config.options.followLinks) {
                    await enqueueLinks({
                        selector: config.rules.linkSelector || 'a[href]',
                        limit: config.options.maxLinks || 10
                    });
                }
            } catch (error) {
                results.push({
                    url: request.loadedUrl,
                    error: error.message,
                    timestamp: new Date().toISOString()
                });
            }
        },
    });
    
    await crawler.run([config.url]);
    
    // 输出结果
    console.log(JSON.stringify({
        success: true,
        results: results,
        total: results.length
    }));
}

async function extractData(page, rules) {
    const data = {};
    
    for (const [key, rule] of Object.entries(rules)) {
        try {
            if (rule.type === 'text') {
                data[key] = await page.textContent(rule.selector);
            } else if (rule.type === 'html') {
                data[key] = await page.innerHTML(rule.selector);
            } else if (rule.type === 'attribute') {
                data[key] = await page.getAttribute(rule.selector, rule.attribute);
            } else if (rule.type === 'list') {
                const elements = await page.$$(rule.selector);
                data[key] = await Promise.all(
                    elements.map(el => el.textContent())
                );
            }
        } catch (error) {
            data[key] = null;
        }
    }
    
    return data;
}

main().catch(console.error);
```

## 部署配置

### 5.1 Docker配置
```dockerfile
# tools-service/Dockerfile
FROM node:18-alpine as node-builder
WORKDIR /app/node_modules
COPY tools/package*.json ./
RUN npm ci --only=production

FROM python:3.11-slim
WORKDIR /app

# 安装Node.js
RUN apt-get update && apt-get install -y curl && \
    curl -fsSL https://deb.nodesource.com/setup_18.x | bash - && \
    apt-get install -y nodejs

# 复制Node.js依赖
COPY --from=node-builder /app/node_modules ./node_modules

# 安装Python依赖
COPY requirements.txt .
RUN pip install -r requirements.txt

# 复制应用代码
COPY . .

EXPOSE 8090
CMD ["python", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8090"]
```

### 5.2 服务注册配置
```python
# tools-service/config/service_config.py
TOOLS_SERVICE_CONFIG = {
    "service_name": "tools-service",
    "service_port": 8090,
    "gateway_registration": {
        "routes": [
            {
                "path": "/api/v1/tools/*",
                "methods": ["GET", "POST", "PUT", "DELETE"]
            }
        ],
        "health_check_path": "/health",
        "load_balancing": "round_robin"
    },
    "tools": {
        "webagent": {
            "enabled": True,
            "config": {
                "search_engines": ["google", "bing", "baidu"],
                "max_results_per_query": 20,
                "timeout": 30
            }
        },
        "crawlee": {
            "enabled": True,
            "config": {
                "max_concurrent": 5,
                "timeout": 120,
                "user_agent": "ZZDSJ-Tools/1.0"
            }
        },
        "scraperr": {
            "enabled": True,
            "config": {
                "max_queue_size": 1000,
                "retry_attempts": 3,
                "rate_limit": "10/minute"
            }
        }
    }
}
```

## 性能和监控

### 6.1 性能优化
- **异步处理**: 所有工具调用使用asyncio
- **连接池**: HTTP客户端使用连接池
- **缓存机制**: 搜索结果和页面内容缓存
- **队列管理**: 大批量任务使用队列异步处理

### 6.2 监控指标
```python
# 关键监控指标
METRICS = {
    "tool_call_duration": "工具调用耗时",
    "tool_success_rate": "工具调用成功率", 
    "concurrent_requests": "并发请求数",
    "queue_length": "队列长度",
    "cache_hit_rate": "缓存命中率",
    "error_rate_by_tool": "各工具错误率"
}
```

## 推荐实施路径

### 阶段一: 核心工具微服务 (2周)
1. 创建tools-service基础架构
2. 集成WebAgent (Python原生)
3. 实现基础的搜索功能
4. 与Agno/LlamaIndex集成测试

### 阶段二: 爬虫能力扩展 (2周)  
1. 集成Crawlee (Node.js桥接)
2. 实现Scraperr核心功能
3. 添加队列和任务管理
4. 性能优化和监控

### 阶段三: 高级功能完善 (1周)
1. 添加智能路由和负载均衡
2. 实现高级配置和管理界面
3. 完善监控和日志
4. 生产环境部署

## 总结

**推荐方案**: 统一工具微服务架构
- **优点**: 统一管理、技术栈一致、易于维护
- **实现**: 主要使用Python，Node.js作为子进程
- **集成**: 通过标准化工具接口与Agno/LlamaIndex集成
- **扩展性**: 易于添加新的工具和功能

这个方案既满足了当前需求，又为未来扩展留下了充分空间。