"""
æµ‹è¯•å¢å¼ºçš„è¯­ä¹‰åˆ†è¯å™¨ç»„ä»¶
éªŒè¯è¿ç§»çš„æ–‡æœ¬å¤„ç†èƒ½åŠ›æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import logging
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.core.text_processor import (
    count_tokens, chunk_text, detect_language, extract_keywords,
    get_text_statistics, clean_text
)
from app.core.chunkers import create_chunker, SmartTextChunker, SemanticChunker
from app.core.tokenizers import create_token_counter, TikTokenCounter, SimpleTokenCounter
from app.core.stop_words_manager import StopWordsManager
from app.core.language_detector import LanguageDetector
from app.core.enhanced_semantic_splitter import EnhancedSemanticSplitter

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_text_processor():
    """æµ‹è¯•æ–‡æœ¬å¤„ç†å™¨"""
    print("ğŸ” æµ‹è¯•æ–‡æœ¬å¤„ç†å™¨...")
    
    test_text = """
    è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯æ–‡æœ¬å¤„ç†åŠŸèƒ½ã€‚
    åŒ…å«ä¸­æ–‡å’ŒEnglishæ··åˆå†…å®¹ã€‚
    
    æˆ‘ä»¬éœ€è¦æµ‹è¯•ä»¥ä¸‹åŠŸèƒ½ï¼š
    1. ä»¤ç‰Œè®¡æ•°
    2. æ–‡æœ¬åˆ†å—
    3. è¯­è¨€æ£€æµ‹
    4. å…³é”®è¯æå–
    5. æ–‡æœ¬ç»Ÿè®¡
    """
    
    try:
        # æµ‹è¯•ä»¤ç‰Œè®¡æ•°
        token_count = count_tokens(test_text)
        print(f"   ğŸ“Š ä»¤ç‰Œæ•°é‡: {token_count}")
        
        # æµ‹è¯•æ–‡æœ¬åˆ†å—
        chunks = chunk_text(test_text, chunk_size=100, chunk_overlap=20)
        print(f"   âœ‚ï¸  æ–‡æœ¬åˆ†å—: {len(chunks)} ä¸ªå—")
        
        # æµ‹è¯•è¯­è¨€æ£€æµ‹
        language = detect_language(test_text)
        print(f"   ğŸŒ æ£€æµ‹è¯­è¨€: {language}")
        
        # æµ‹è¯•å…³é”®è¯æå–
        keywords = extract_keywords(test_text, max_keywords=5)
        print(f"   ğŸ”‘ å…³é”®è¯: {keywords}")
        
        # æµ‹è¯•æ–‡æœ¬ç»Ÿè®¡
        stats = get_text_statistics(test_text)
        print(f"   ğŸ“ˆ æ–‡æœ¬ç»Ÿè®¡: {stats}")
        
        # æµ‹è¯•æ–‡æœ¬æ¸…ç†
        cleaned = clean_text(test_text)
        print(f"   ğŸ§¹ æ¸…ç†åé•¿åº¦: {len(cleaned)}")
        
        print("   âœ… æ–‡æœ¬å¤„ç†å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ æ–‡æœ¬å¤„ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_chunkers():
    """æµ‹è¯•åˆ†å—å™¨"""
    print("ğŸ” æµ‹è¯•åˆ†å—å™¨...")
    
    test_text = """
    äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å‘å±•æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œã€‚æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ç®—æ³•ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚
    
    è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚å®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
    
    è®¡ç®—æœºè§†è§‰æŠ€æœ¯è®©æœºå™¨èƒ½å¤Ÿ"çœ‹è§"å’Œç†è§£å›¾åƒã€‚è¿™é¡¹æŠ€æœ¯åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚
    """
    
    try:
        from app.core.text_base import ChunkConfig
        
        config = ChunkConfig(chunk_size=150, chunk_overlap=30)
        
        # æµ‹è¯•æ™ºèƒ½åˆ†å—å™¨
        smart_chunker = SmartTextChunker(config)
        smart_chunks = smart_chunker.chunk(test_text)
        print(f"   ğŸ§  æ™ºèƒ½åˆ†å—å™¨: {len(smart_chunks)} ä¸ªå—")
        
        # æµ‹è¯•è¯­ä¹‰åˆ†å—å™¨
        semantic_chunker = SemanticChunker(config)
        semantic_chunks = semantic_chunker.chunk(test_text)
        print(f"   ğŸ¯ è¯­ä¹‰åˆ†å—å™¨: {len(semantic_chunks)} ä¸ªå—")
        
        # æµ‹è¯•å·¥å‚å‡½æ•°
        factory_chunker = create_chunker("smart", config)
        factory_chunks = factory_chunker.chunk(test_text)
        print(f"   ğŸ­ å·¥å‚åˆ†å—å™¨: {len(factory_chunks)} ä¸ªå—")
        
        print("   âœ… åˆ†å—å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ åˆ†å—å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_tokenizers():
    """æµ‹è¯•åˆ†è¯å™¨"""
    print("ğŸ” æµ‹è¯•åˆ†è¯å™¨...")
    
    test_text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ï¼ŒåŒ…å«ä¸­æ–‡å’ŒEnglishå†…å®¹ã€‚"
    
    try:
        # æµ‹è¯•ç®€å•è®¡æ•°å™¨
        simple_counter = SimpleTokenCounter({})
        await simple_counter.initialize()
        
        simple_tokens = await simple_counter.tokenize(test_text, 'zh')
        print(f"   ğŸ”¤ ç®€å•åˆ†è¯å™¨: {len(simple_tokens)} ä¸ªtoken")
        
        # æµ‹è¯•TikTokenè®¡æ•°å™¨
        tiktoken_counter = TikTokenCounter({'model': 'gpt-3.5-turbo'})
        await tiktoken_counter.initialize()
        
        tiktoken_tokens = await tiktoken_counter.tokenize(test_text, 'zh')
        print(f"   ğŸ¯ TikTokenåˆ†è¯å™¨: {len(tiktoken_tokens)} ä¸ªtoken")
        
        # æµ‹è¯•å·¥å‚å‡½æ•°
        factory_counter = create_token_counter(use_tiktoken=False)
        token_count = factory_counter.count_tokens(test_text)
        print(f"   ğŸ­ å·¥å‚è®¡æ•°å™¨: {token_count} ä¸ªtoken")
        
        print("   âœ… åˆ†è¯å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_stop_words_manager():
    """æµ‹è¯•åœç”¨è¯ç®¡ç†å™¨"""
    print("ğŸ” æµ‹è¯•åœç”¨è¯ç®¡ç†å™¨...")
    
    try:
        config = {
            'enable_builtin': True,
            'enable_domain_specific': True,
            'enable_custom': True,
            'sources': [
                {
                    'type': 'builtin',
                    'languages': ['zh', 'en']
                }
            ]
        }
        
        manager = StopWordsManager(config)
        await manager.initialize()
        
        # æµ‹è¯•åœç”¨è¯æ£€æµ‹
        is_stop_zh = await manager.is_stop_word('çš„', 'zh')
        is_stop_en = await manager.is_stop_word('the', 'en')
        is_not_stop = await manager.is_stop_word('äººå·¥æ™ºèƒ½', 'zh')
        
        print(f"   ğŸš« 'çš„' æ˜¯åœç”¨è¯: {is_stop_zh}")
        print(f"   ğŸš« 'the' æ˜¯åœç”¨è¯: {is_stop_en}")
        print(f"   âœ… 'äººå·¥æ™ºèƒ½' æ˜¯åœç”¨è¯: {is_not_stop}")
        
        # æµ‹è¯•è‡ªå®šä¹‰åœç”¨è¯
        await manager.add_custom_stop_words(['æµ‹è¯•', 'test'], 'zh')
        is_custom_stop = await manager.is_stop_word('æµ‹è¯•', 'zh')
        print(f"   ğŸ”§ è‡ªå®šä¹‰åœç”¨è¯ 'æµ‹è¯•': {is_custom_stop}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = manager.get_statistics()
        print(f"   ğŸ“Š åœç”¨è¯ç»Ÿè®¡: {stats['total_languages']} ç§è¯­è¨€")
        
        await manager.cleanup()
        print("   âœ… åœç”¨è¯ç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ åœç”¨è¯ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_language_detector():
    """æµ‹è¯•è¯­è¨€æ£€æµ‹å™¨"""
    print("ğŸ” æµ‹è¯•è¯­è¨€æ£€æµ‹å™¨...")
    
    try:
        config = {
            'primary_threshold': 0.8,
            'mixed_language_threshold': 0.3,
            'supported_languages': ['zh', 'en', 'ja', 'ko']
        }
        
        detector = LanguageDetector(config)
        
        # æµ‹è¯•ä¸­æ–‡æ£€æµ‹
        zh_text = "è¿™æ˜¯ä¸€æ®µä¸­æ–‡æ–‡æœ¬ï¼Œç”¨äºæµ‹è¯•è¯­è¨€æ£€æµ‹åŠŸèƒ½ã€‚"
        zh_info = await detector.detect_language(zh_text)
        print(f"   ğŸ‡¨ğŸ‡³ ä¸­æ–‡æ£€æµ‹: {zh_info.primary_language} (ç½®ä¿¡åº¦: {zh_info.confidence:.2f})")
        
        # æµ‹è¯•è‹±æ–‡æ£€æµ‹
        en_text = "This is an English text for testing language detection."
        en_info = await detector.detect_language(en_text)
        print(f"   ğŸ‡ºğŸ‡¸ è‹±æ–‡æ£€æµ‹: {en_info.primary_language} (ç½®ä¿¡åº¦: {en_info.confidence:.2f})")
        
        # æµ‹è¯•æ··åˆè¯­è¨€æ£€æµ‹
        mixed_text = "è¿™æ˜¯ä¸­æ–‡ã€‚This is English. è¿™åˆæ˜¯ä¸­æ–‡äº†ã€‚"
        mixed_segments = await detector.detect_mixed_languages(mixed_text)
        print(f"   ğŸŒ æ··åˆè¯­è¨€åˆ†æ®µ: {len(mixed_segments)} ä¸ªæ®µè½")
        
        for i, segment in enumerate(mixed_segments):
            print(f"      æ®µè½{i+1}: {segment.language} - {segment.text[:20]}...")
        
        print("   âœ… è¯­è¨€æ£€æµ‹å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ è¯­è¨€æ£€æµ‹å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_enhanced_semantic_splitter():
    """æµ‹è¯•å¢å¼ºçš„è¯­ä¹‰åˆ†å‰²å™¨"""
    print("ğŸ” æµ‹è¯•å¢å¼ºçš„è¯­ä¹‰åˆ†å‰²å™¨...")
    
    try:
        config = {
            'min_chunk_size': 100,
            'max_chunk_size': 300,
            'coherence_threshold': 0.7,
            'stop_words': {
                'enable_builtin': True,
                'sources': [
                    {
                        'type': 'builtin',
                        'languages': ['zh', 'en']
                    }
                ]
            },
            'language_detection': {
                'primary_threshold': 0.8,
                'supported_languages': ['zh', 'en']
            },
            'tokenizers': {
                'use_tiktoken': False,
                'token_counter': {'model': 'gpt-3.5-turbo'}
            }
        }
        
        splitter = EnhancedSemanticSplitter(config)
        await splitter.initialize()
        
        test_text = """
        äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ã€‚æœºå™¨å­¦ä¹ ç®—æ³•ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»å¤§é‡æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚
        
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚å®ƒæ¨¡ä»¿äººè„‘ç¥ç»ç½‘ç»œçš„ç»“æ„ï¼Œé€šè¿‡å¤šå±‚ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ•°æ®ã€‚
        
        è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è¿™é¡¹æŠ€æœ¯åœ¨æœç´¢å¼•æ“ã€æ™ºèƒ½åŠ©æ‰‹ã€æœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸæœ‰å¹¿æ³›åº”ç”¨ã€‚
        
        è®¡ç®—æœºè§†è§‰æŠ€æœ¯ä½¿æœºå™¨èƒ½å¤Ÿè¯†åˆ«å’Œç†è§£å›¾åƒå†…å®¹ã€‚åœ¨è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—è¯Šæ–­ã€å®‰é˜²ç›‘æ§ç­‰é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚
        """
        
        chunks, processing_time = await splitter.split_with_timing(test_text)
        
        print(f"   âš¡ å¤„ç†æ—¶é—´: {processing_time:.3f}ç§’")
        print(f"   ğŸ“„ ç”Ÿæˆåˆ†å—: {len(chunks)} ä¸ª")
        
        for i, chunk in enumerate(chunks):
            print(f"   å—{i+1}: {len(chunk.content)} å­—ç¬¦, è¿è´¯æ€§: {chunk.metadata.get('coherence_score', 0):.2f}")
            print(f"      è¯­è¨€: {chunk.metadata.get('language', 'unknown')}")
            print(f"      å…³é”®è¯: {chunk.semantic_info.get('keywords', [])[:3]}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = splitter.get_statistics(chunks)
        print(f"   ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: å¹³å‡é•¿åº¦ {stats['avg_chunk_length']:.0f}, å¹³å‡è¿è´¯æ€§ {stats['avg_coherence_score']:.2f}")
        
        await splitter.cleanup()
        print("   âœ… å¢å¼ºè¯­ä¹‰åˆ†å‰²å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"   âŒ å¢å¼ºè¯­ä¹‰åˆ†å‰²å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•å¢å¼ºçš„è¯­ä¹‰åˆ†è¯å™¨ç»„ä»¶")
    print("=" * 50)
    
    tests = [
        ("æ–‡æœ¬å¤„ç†å™¨", test_text_processor),
        ("åˆ†å—å™¨", test_chunkers),
        ("åˆ†è¯å™¨", test_tokenizers),
        ("åœç”¨è¯ç®¡ç†å™¨", test_stop_words_manager),
        ("è¯­è¨€æ£€æµ‹å™¨", test_language_detector),
        ("å¢å¼ºè¯­ä¹‰åˆ†å‰²å™¨", test_enhanced_semantic_splitter),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ æµ‹è¯• {test_name}:")
        try:
            result = await test_func()
            if result:
                passed += 1
        except Exception as e:
            print(f"   âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
    
    print("\n" + "=" * 50)
    print(f"ğŸ¯ æµ‹è¯•å®Œæˆ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è¿ç§»çš„ç»„ä»¶å·¥ä½œæ­£å¸¸ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥ç›¸å…³ç»„ä»¶ã€‚")

if __name__ == "__main__":
    asyncio.run(main())