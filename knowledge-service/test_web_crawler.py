#!/usr/bin/env python3
"""
Web CrawleråŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•URLå¯¼å…¥çˆ¬è™«çš„åŸºæœ¬åŠŸèƒ½
"""

import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.core.web_crawler_manager import WebCrawlerManager, CrawlConfig, CrawlMode
from app.models.database import get_db


async def test_basic_crawler():
    """æµ‹è¯•åŸºç¡€çˆ¬è™«åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•Web CrawleråŸºç¡€åŠŸèƒ½")
    print("=" * 60)
    
    # è·å–æ•°æ®åº“è¿æ¥
    db = next(get_db())
    
    try:
        # æµ‹è¯•URLåˆ—è¡¨
        test_urls = [
            "https://httpbin.org/html",  # ç®€å•HTMLæµ‹è¯•
            "https://httpbin.org/json",  # JSONå“åº”æµ‹è¯•
        ]
        
        # åˆ›å»ºçˆ¬è™«é…ç½®
        config = CrawlConfig(
            mode=CrawlMode.URL_LIST,
            max_pages=5,
            concurrent_requests=2,
            request_delay=1.0,
            use_llamaindex=False,  # å…ˆæµ‹è¯•åŸºç¡€åŠŸèƒ½
            use_trafilatura=False,
            min_content_length=10,  # é™ä½è¦æ±‚ä¾¿äºæµ‹è¯•
            max_content_length=50000,
            include_metadata=True
        )
        
        print(f"æµ‹è¯•URL: {test_urls}")
        print(f"çˆ¬è™«é…ç½®: {config.mode.value}, æœ€å¤§é¡µé¢: {config.max_pages}")
        print("-" * 40)
        
        # æ‰§è¡Œçˆ¬è™«æµ‹è¯•
        async with WebCrawlerManager(db) as crawler:
            result = await crawler.crawl_urls(
                kb_id="test-kb-001",
                urls=test_urls,
                config=config
            )
        
        # è¾“å‡ºç»“æœ
        print("çˆ¬è™«æ‰§è¡Œç»“æœ:")
        print(f"æˆåŠŸ: {result['success']}")
        print(f"æ€»URLæ•°: {result['total_urls']}")
        print(f"æˆåŠŸæ•°: {result['successful_count']}")
        print(f"å¤±è´¥æ•°: {result['failed_count']}")
        print("-" * 40)
        
        # å±•ç¤ºæˆåŠŸç»“æœ
        for i, crawl_result in enumerate(result['results'][:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
            print(f"\nç»“æœ {i+1}:")
            print(f"URL: {crawl_result.url}")
            print(f"æ ‡é¢˜: {crawl_result.title}")
            print(f"å†…å®¹é•¿åº¦: {len(crawl_result.content)}")
            print(f"Markdowné•¿åº¦: {len(crawl_result.markdown_content)}")
            print(f"çŠ¶æ€: {crawl_result.status}")
            print(f"çˆ¬å–æ—¶é—´: {crawl_result.crawl_time:.2f}s")
            print(f"å†…å®¹é¢„è§ˆ: {crawl_result.content[:200]}...")
            
        # å±•ç¤ºå¤±è´¥ç»“æœ
        if result['failed_results']:
            print(f"\nå¤±è´¥ç»“æœ ({len(result['failed_results'])}ä¸ª):")
            for failed in result['failed_results'][:2]:
                print(f"URL: {failed.url}")
                print(f"é”™è¯¯: {failed.error_message}")
        
        print("\n" + "=" * 60)
        print("åŸºç¡€çˆ¬è™«æµ‹è¯•å®Œæˆ")
        
        return result['success']
        
    except Exception as e:
        print(f"çˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        db.close()


async def test_llamaindex_crawler():
    """æµ‹è¯•LlamaIndexé›†æˆçˆ¬è™«"""
    print("=" * 60)
    print("æµ‹è¯•LlamaIndexé›†æˆçˆ¬è™«")
    print("=" * 60)
    
    db = next(get_db())
    
    try:
        # æµ‹è¯•URL
        test_urls = ["https://docs.python.org/3/tutorial/introduction.html"]
        
        # å¯ç”¨LlamaIndexçš„é…ç½®
        config = CrawlConfig(
            mode=CrawlMode.SINGLE_URL,
            use_llamaindex=True,
            use_trafilatura=True,
            min_content_length=100,
            include_metadata=True
        )
        
        print(f"æµ‹è¯•URL: {test_urls[0]}")
        print("ä½¿ç”¨LlamaIndex + Trafilaturaæå–")
        print("-" * 40)
        
        async with WebCrawlerManager(db) as crawler:
            result = await crawler.crawl_urls(
                kb_id="test-kb-002",
                urls=test_urls,
                config=config
            )
        
        if result['success'] and result['results']:
            crawl_result = result['results'][0]
            print("LlamaIndexçˆ¬è™«ç»“æœ:")
            print(f"URL: {crawl_result.url}")
            print(f"æ ‡é¢˜: {crawl_result.title}")
            print(f"å†…å®¹é•¿åº¦: {len(crawl_result.content)}")
            print(f"Markdowné•¿åº¦: {len(crawl_result.markdown_content)}")
            print(f"çŠ¶æ€: {crawl_result.status}")
            print(f"å…ƒæ•°æ®: {json.dumps(crawl_result.metadata, indent=2, ensure_ascii=False)}")
            print("\nMarkdownå†…å®¹é¢„è§ˆ:")
            print(crawl_result.markdown_content[:500] + "...")
        else:
            print("LlamaIndexçˆ¬è™«æ‰§è¡Œå¤±è´¥")
            if result.get('failed_results'):
                print(f"é”™è¯¯: {result['failed_results'][0].error_message}")
        
        print("\n" + "=" * 60)
        print("LlamaIndexçˆ¬è™«æµ‹è¯•å®Œæˆ")
        
        return result['success']
        
    except Exception as e:
        print(f"LlamaIndexçˆ¬è™«æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        db.close()


async def test_content_filtering():
    """æµ‹è¯•å†…å®¹è¿‡æ»¤åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•å†…å®¹è¿‡æ»¤åŠŸèƒ½")
    print("=" * 60)
    
    db = next(get_db())
    
    try:
        # æµ‹è¯•åŒ…å«å¤šç§å…ƒç´ çš„HTMLé¡µé¢
        test_urls = ["https://httpbin.org/html"]
        
        # é…ç½®å†…å®¹è¿‡æ»¤å™¨
        config = CrawlConfig(
            mode=CrawlMode.SINGLE_URL,
            use_llamaindex=False,
            content_filters=["script", "style", "nav"],  # è¿‡æ»¤è¿™äº›å…ƒç´ 
            content_selectors=["body"],  # åªä¿ç•™bodyå†…å®¹
            min_content_length=10,
            include_metadata=True
        )
        
        print(f"æµ‹è¯•URL: {test_urls[0]}")
        print(f"å†…å®¹è¿‡æ»¤å™¨: {config.content_filters}")
        print(f"å†…å®¹é€‰æ‹©å™¨: {config.content_selectors}")
        print("-" * 40)
        
        async with WebCrawlerManager(db) as crawler:
            result = await crawler.crawl_urls(
                kb_id="test-kb-003",
                urls=test_urls,
                config=config
            )
        
        if result['success'] and result['results']:
            crawl_result = result['results'][0]
            print("å†…å®¹è¿‡æ»¤ç»“æœ:")
            print(f"åŸå§‹å†…å®¹é•¿åº¦: {len(crawl_result.content)}")
            print(f"æ¸…æ´—åå†…å®¹é¢„è§ˆ:")
            print(crawl_result.content[:300] + "...")
            print("\nMarkdownæ ¼å¼:")
            print(crawl_result.markdown_content[:300] + "...")
        else:
            print("å†…å®¹è¿‡æ»¤æµ‹è¯•å¤±è´¥")
        
        print("\n" + "=" * 60)
        print("å†…å®¹è¿‡æ»¤æµ‹è¯•å®Œæˆ")
        
        return result['success']
        
    except Exception as e:
        print(f"å†…å®¹è¿‡æ»¤æµ‹è¯•å¤±è´¥: {e}")
        return False
    finally:
        db.close()


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹Web Crawlerå®Œæ•´åŠŸèƒ½æµ‹è¯•")
    print("å½“å‰å·¥ä½œç›®å½•:", Path.cwd())
    print("é¡¹ç›®æ ¹ç›®å½•:", project_root)
    
    # æµ‹è¯•è®¡æ•°å™¨
    total_tests = 0
    passed_tests = 0
    
    # æµ‹è¯•1: åŸºç¡€çˆ¬è™«åŠŸèƒ½
    total_tests += 1
    if await test_basic_crawler():
        passed_tests += 1
        print("âœ… åŸºç¡€çˆ¬è™«æµ‹è¯•é€šè¿‡")
    else:
        print("âŒ åŸºç¡€çˆ¬è™«æµ‹è¯•å¤±è´¥")
    
    print("\n" + "â³ ç­‰å¾…2ç§’...")
    await asyncio.sleep(2)
    
    # æµ‹è¯•2: LlamaIndexé›†æˆï¼ˆå¯èƒ½å› ä¸ºä¾èµ–é—®é¢˜å¤±è´¥ï¼‰
    total_tests += 1
    if await test_llamaindex_crawler():
        passed_tests += 1
        print("âœ… LlamaIndexçˆ¬è™«æµ‹è¯•é€šè¿‡")
    else:
        print("âŒ LlamaIndexçˆ¬è™«æµ‹è¯•å¤±è´¥ï¼ˆå¯èƒ½å› ä¸ºä¾èµ–ç¼ºå¤±ï¼‰")
    
    print("\n" + "â³ ç­‰å¾…2ç§’...")
    await asyncio.sleep(2)
    
    # æµ‹è¯•3: å†…å®¹è¿‡æ»¤
    total_tests += 1
    if await test_content_filtering():
        passed_tests += 1
        print("âœ… å†…å®¹è¿‡æ»¤æµ‹è¯•é€šè¿‡")
    else:
        print("âŒ å†…å®¹è¿‡æ»¤æµ‹è¯•å¤±è´¥")
    
    # è¾“å‡ºæµ‹è¯•æ€»ç»“
    print("\n" + "=" * 80)
    print("Web Crawleræµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
    print(f"å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
    print(f"é€šè¿‡ç‡: {passed_tests/total_tests*100:.1f}%")
    
    if passed_tests == total_tests:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Web CrawleråŠŸèƒ½æ­£å¸¸")
        return 0
    elif passed_tests > 0:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼ŒWeb CrawleråŸºæœ¬åŠŸèƒ½å¯ç”¨")
        return 0
    else:
        print("âŒ æ‰€æœ‰æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œä¾èµ–")
        return 1


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\næµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)